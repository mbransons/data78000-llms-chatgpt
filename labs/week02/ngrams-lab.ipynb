{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51f17aa",
   "metadata": {},
   "source": [
    "## Ngrams lab\n",
    "LLM's and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
    "\n",
    "**Due:** September 17\n",
    "\n",
    "\n",
    "### Background\n",
    "The purpose of this lab is to explore ngram models. Ngram models are a good introduction to language models generally. Language models are probabilistic representations of language. Ngrams have the benefit of being easy to interrogate and relatively easy to understand (as compared to neural networks). \n",
    "\n",
    "In this lab, you will build an ngram model from the corpus of your choosing. The example is with 'The Great Gatsby' from Project Gutenberg, but there's a code block for any text file on your computer  \n",
    "\n",
    "\n",
    "#### Notes\n",
    "This lab is based heavily on the [nltk documentation](https://www.nltk.org/api/nltk.lm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef35858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# if you haven't downloaded punkt before, you only need to run the line below once \n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7017636",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "An example of how ngrams are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8054bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Great Gatsby        This ebook is for the use of anyone anywhere\n"
     ]
    }
   ],
   "source": [
    "# you will need to leverage the requests package\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
    "great_gatsby = r.text\n",
    "\n",
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    great_gatsby = great_gatsby.replace(char, \" \")\n",
    "\n",
    "# check\n",
    "print(great_gatsby[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "886ac257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the East last autumn I felt that I wanted  the world to be in uniform and at a sort of moral attenti\n"
     ]
    }
   ],
   "source": [
    "# remove the metadata at the beginning - this is slightly different for each book\n",
    "great_gatsby = great_gatsby[983:]\n",
    "print(great_gatsby[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39a39e",
   "metadata": {},
   "source": [
    "#### Txt locally\n",
    "If you'd rather use a file on your computer, here's the code -- you just need to save the text file in your local directory, and change the variables throughout. \n",
    "\n",
    "The example is a report from the [Congressional Research Service](https://www.everycrsreport.com/files/2020-11-10_R45178_62d6238caecf6c02ddf495be33b3439f09eed744.pdf) on AI and National Security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a465698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Chapter I The Cyclone   Dorothy lived in the midst of the great Kansas prairies, with Uncle Hen\n"
     ]
    }
   ],
   "source": [
    "f = open(\"the-wizard-of-oz.txt\", 'r').read()\n",
    "\n",
    "for char in ['\\n', '\\r', '\\d', '\\t']:\n",
    "    f = f.replace(char, ' ')\n",
    "\n",
    "wiz_of_oz = f[3110:]\n",
    "print(wiz_of_oz[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c4c9736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', 'i', 'the', 'cyclone', 'dorothy', 'lived', 'in', 'the', 'midst', 'of', 'the', 'great', 'kansas', 'prairies', 'with', 'uncle', 'henry', 'who', 'was', 'a', 'farmer', 'and', 'aunt', 'em', 'who', 'was', 'the', 'farmers', 'wife', 'their', 'house', 'was', 'small', 'for', 'the', 'lumber', 'to', 'build', 'it', 'had', 'to', 'be', 'carried', 'by', 'wagon', 'many', 'miles', 'there', 'were', 'four']\n"
     ]
    }
   ],
   "source": [
    "# this is simplified for demonstration\n",
    "def sample_clean_text(text: str):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation from text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # return your tokens\n",
    "    return tokens\n",
    "\n",
    "# call the function\n",
    "sample_tokens = sample_clean_text(text = wiz_of_oz)\n",
    "\n",
    "# check\n",
    "print(sample_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94d828dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chapter', 'i'),\n",
       " ('i', 'the'),\n",
       " ('the', 'cyclone'),\n",
       " ('cyclone', 'dorothy'),\n",
       " ('dorothy', 'lived'),\n",
       " ('lived', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'midst'),\n",
       " ('midst', 'of'),\n",
       " ('of', 'the')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bigrams from the sample tokens\n",
    "my_bigrams = bigrams(sample_tokens)\n",
    "\n",
    "# check\n",
    "list(my_bigrams)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239deb54",
   "metadata": {},
   "source": [
    "# Part 2 - creating an ngram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9aba6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 is for bigrams\n",
    "n = 2\n",
    "#specify the text you want to use\n",
    "text = wiz_of_oz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be61c83",
   "metadata": {},
   "source": [
    "Now we are going to use an NLTK shortcut for preprocessing. This will:\n",
    "* pad all of the sentences with `<s>` and `</s>` to train on sentence boundaries, too.\n",
    "* create both unigrams and bigrams\n",
    "* create a training set and a full vocab to train on\n",
    "\n",
    "We need to give it a pre-tokenized text (we'll use nltk's tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6276da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', 'i', 'the', 'cyclone', 'dorothy', 'lived', 'in', 'the', 'midst', 'of', 'the', 'great', 'kansas', 'prairies', ',', 'with', 'uncle', 'henry', ',', 'who', 'was', 'a', 'farmer', ',', 'and', 'aunt', 'em', ',', 'who', 'was', 'the', 'farmer', '’', 's', 'wife', '.']\n"
     ]
    }
   ],
   "source": [
    "# step 1: tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# step 2: tokenize each sentence into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# step 3: convert each word to lowercase\n",
    "tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
    "\n",
    "#notice the sentence breaks and what the first 10 items of the tokenized text\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7a3de",
   "metadata": {},
   "source": [
    "Why tokenize sentences and words?\n",
    "We want to be able to retain sentence boundaries to encode that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fbc5554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Chapter I The Cyclone   Dorothy lived in the midst of the great Kansas prairies, with Uncle Henry\n"
     ]
    }
   ],
   "source": [
    "# notice what the first 10 items are of the vocabulary\n",
    "print(text[:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c306af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we imported this function from nltk\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca61c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "# we imported this function from nltk linear models (lm) \n",
    "# it is for Maximum Likelihood Estimation\n",
    "\n",
    "# MLE is the model we will use\n",
    "lm = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3ecfc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently the vocab length is 0: it has no prior knowledge\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "141795d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3574"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model \n",
    "# training data is the bigrams and unigrams \n",
    "# the vocab is all the sentence tokens in the corpus \n",
    "\n",
    "lm.fit(train_data, padded_sents)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "941458b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chapter', 'i', 'the', 'cyclone', 'dorothy', 'lived', 'in', 'the', 'midst', 'of', 'the', 'great', 'kansas', 'prairies', ',', 'with', 'uncle', 'henry', ',', 'who', 'was', 'a', 'farmer', ',', 'and', 'aunt', 'em', ',', 'who', 'was', 'the', 'farmer', '’', 's', 'wife', '.')\n"
     ]
    }
   ],
   "source": [
    "# inspect the model's vocabulary. \n",
    "# be sure that a sentence you know exists (from tokenized_text) is in the \n",
    "print(lm.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d820f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('then', 'wear', 'the', 'gold', 'hat', '<UNK>', '.')\n"
     ]
    }
   ],
   "source": [
    "# see what happens when we include a word that is not in the vocab. \n",
    "print(lm.vocab.lookup('then wear the gold hat iphone .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207d680",
   "metadata": {},
   "source": [
    "What did the model replace 'iphone' with? \n",
    "\n",
    "Given that it didn't just return an \"out of vocab\" error, what does that mean about our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da9cf2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0068670009729810645"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times does dorothy appear in the model?\n",
    "print(lm.counts['dorothy'])\n",
    "\n",
    "# what is the probability of dorothy appearing? \n",
    "# this is technically the relative frequency of dorothy appearing \n",
    "lm.score('dorothy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9369bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05449591280653951"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how often does (dorothy, and) occur and what is the relative frequency?\n",
    "print(lm.counts[['dorothy']]['and'])\n",
    "lm.score('and', 'dorothy'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "174f226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the score of 'UNK'? \n",
    "\n",
    "lm.score(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c0444",
   "metadata": {},
   "source": [
    "Does the relative frequency of 'UNK' change your assumption about how the model behaves? \n",
    "\n",
    "How should we change our model to account for the fact the `<UNK>` words are not accounted for by the model?\n",
    "\n",
    "Note: *Programmatically implementing this solution is beyond the scope of this course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d621b05",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "We want to start our sentence with a word, and use that to predict all the words that come after that. We'll specify how long it should be. \n",
    "\n",
    "There is a certain amount of randomness encoded into n-gram models. This prevents a model from becoming entirely deterministic. Maximum Likelihood Estimation without some degree of randomness will only produce the most likely result every time. Setting Random Seed means we will get the same result every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e217db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'little', 'black', 'eyes', ',', 'who', 'the', 'trees', 'were', 'everywhere', ',', 'i', 'am', 'stuffed', 'man', 'who', 'had', 'been', 'afraid', 'we']\n"
     ]
    }
   ],
   "source": [
    "# generate a 20 word sentence starting with the word, 'dorothy'\n",
    "\n",
    "print(lm.generate(20, text_seed= 'dorothy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91b7a6",
   "metadata": {},
   "source": [
    "This next code block is just to clean up the tokenized words and make them easier on human eyes. It is literally a detokenizer, which removes some extraneous text markup and reconciles some words back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5113674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(lm, num_words, text_seed, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in lm.generate(num_words, text_seed=text_seed, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2846e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me, while his tears by the emerald green to'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now generate sentences that look much nicer. \n",
    "generate_sent(lm, 10, text_seed='dorothy', random_seed = 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ad126",
   "metadata": {},
   "source": [
    "Try a few more sentences, and try out another text. Once you are satisfied with what ngrams can (and cannot) do - post your code to your Github or another site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "87b321bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out the means of various formats will oil was a coward to the girl lived.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(lm, 20, text_seed='dorothy', random_seed = 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3beff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electronic works even if the lion, however, and get to live in this way off all ready.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(lm, 20, text_seed='dorothy', random_seed = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a8e20bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'little man spoke to the laughter that is fit better'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(lm, 10, text_seed='dorothy', random_seed = 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ba6bf",
   "metadata": {},
   "source": [
    "## Parse Lines from SRT files\n",
    "\n",
    "modified from [pablo-var/learn-english-words-from-srt](https://github.com/pablo-var/learn-english-words-from-srt) repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7e818b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_time_stamp(l):\n",
    "  if l[:2].isnumeric() and l[2] == ':':\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def has_letters(line):\n",
    "  if re.search('[a-zA-Z]', line):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "\n",
    "def has_no_text(line):\n",
    "  l = line.strip()\n",
    "  if not len(l):\n",
    "    return True\n",
    "  if l.isnumeric():\n",
    "    return True\n",
    "  if is_time_stamp(l):\n",
    "    return True\n",
    "  if l[0] == '(' and l[-1] == ')':\n",
    "    return True\n",
    "  if not has_letters(line):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def is_lowercase_letter_or_comma(letter):\n",
    "  if letter.isalpha() and letter.lower() == letter:\n",
    "    return True\n",
    "  if letter == ',':\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def clean_up(lines):\n",
    "  \"\"\"\n",
    "  Get rid of all non-text lines and\n",
    "  try to combine text broken into multiple lines\n",
    "  \"\"\"\n",
    "  new_lines = []\n",
    "  for line in lines[1:]:\n",
    "    line = remove_non_ascii(line)\n",
    "    for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "        line = line.replace(char, \"\")\n",
    "    if has_no_text(line):\n",
    "      continue\n",
    "    elif len(new_lines) and is_lowercase_letter_or_comma(line[0]):\n",
    "      #combine with previous line\n",
    "      new_lines[-1] = new_lines[-1].strip() + ' ' + line\n",
    "    else:\n",
    "      #append line\n",
    "      new_lines.append(line)\n",
    "  return new_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56296a",
   "metadata": {},
   "source": [
    "Sample sentence tokenization of 'A Nightmare on Elm Street' srt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c9a6f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tina.', 'Tina.', 'Tina.', 'Tina. Tina.', 'You okay, Tina?', 'Just a dream, Ma.', 'Some dream, judging from that.', '- You coming back to the sack or what?', '- Hold your horses.', 'Tina, honey, you gotta cut your fingernails or you gotta stop that kind of dreaming.', 'One or the other.', 'One, two', \"Freddy's coming for you\", 'Three, four', 'Better lock your door', 'Five, six', 'Grab your crucifix', 'Seven, eight', 'Gonna stay up late', 'Nine, ten']\n"
     ]
    }
   ],
   "source": [
    "elm_street = open(\"1984-srts/A_Nightmare_on_Elm_Street-1984.srt\", 'r').readlines()\n",
    "elm_clean = clean_up(elm_street)\n",
    "\n",
    "print(elm_clean[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779e473",
   "metadata": {},
   "source": [
    "sample word tokenization of tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "76e398bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'or', 'the', 'other', '.']\n"
     ]
    }
   ],
   "source": [
    "# step 2: tokenize each sentence into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in elm_clean]\n",
    "\n",
    "# step 3: convert each word to lowercase\n",
    "tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
    "\n",
    "#notice the sentence breaks and what the first 10 items of the tokenized text\n",
    "print(tokenized_text[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb52dc",
   "metadata": {},
   "source": [
    "All SRT files of American films from 1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "66f68aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the files\n",
    "directory = '1984-srts'\n",
    "\n",
    "# Initialize an empty list to store the combined lines from all files\n",
    "combined_lines = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory) and os.path.isdir(directory):\n",
    "    # List all files in the directory\n",
    "    file_list = os.listdir(directory)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in file_list:\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                # Open the file and read its lines\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    lines = file.readlines()\n",
    "\n",
    "                # Clean up the lines using your clean_up function\n",
    "                cleaned_lines = clean_up(lines)\n",
    "\n",
    "                # Extend the combined_lines list with the cleaned lines from the current file\n",
    "                combined_lines.extend(cleaned_lines)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading or processing file {file_path}: {e}\")\n",
    "\n",
    "# Now, combined_lines contains all the cleaned-up lines from all the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7475d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17', 'years', 'and', '15', 'albums', 'later', ',']\n"
     ]
    }
   ],
   "source": [
    "# step 2: tokenize each sentence into words\n",
    "tokenized_lines = [nltk.word_tokenize(sent) for sent in combined_lines]\n",
    "\n",
    "# step 3: convert each word to lowercase\n",
    "tokenized_words = [[word.lower() for word in sent] for sent in tokenized_lines]\n",
    "\n",
    "#notice the sentence breaks and what the first 10 items of the tokenized text\n",
    "print(tokenized_words[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "713e2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2, padded_sents2 = padded_everygram_pipeline(n, tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8a9dee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37029"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = MLE(n)\n",
    "lm2.fit(train_data2, padded_sents2)\n",
    "len(lm2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "364e6d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00044021498021316457"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times does dorothy appear in the model?\n",
    "print(lm2.counts['money'])\n",
    "\n",
    "# what is the probability of dorothy appearing? \n",
    "# this is technically the relative frequency of dorothy appearing \n",
    "lm2.score('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3011aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', '</s>', 'outweighs', 'me', 'what', 'you', 'up', '.', '</s>', 'important', 'to', 'move', 'the', 'problem', '?', '</s>', '3', '.', '</s>', \"'ll\"]\n"
     ]
    }
   ],
   "source": [
    "print(lm.generate(20, text_seed= 'money'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0c878a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': i tell him.'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(lm2, 20, text_seed='money', random_seed = 44)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
