{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b60958",
   "metadata": {},
   "source": [
    "## Wordvectors lab\n",
    "LLM's and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
    "\n",
    "**Due:** October 8\n",
    "\n",
    "* **DOWNLOAD** [Embeddings file for Part 2 here](https://drive.google.com/file/d/1NQ_7ERV3eiUnPCoIW7I8yi4TZMiu2cEQ/view?usp=sharing)\n",
    "* Unzip the file\n",
    "* Move it to the folder where you are writing your code. \n",
    "\n",
    "\n",
    "### Background\n",
    "The purpose of this lab is to explore the process of transforming words into numerical representations, and then understanding those values spatially. By the end of this lab (and corresponding lecture), you should be comfortable with the fact that computers transform text into numbers based on the context in which they appear. \n",
    "\n",
    "There are **two** parts to this lab:\n",
    "\n",
    "* **Part 1**: We'll train our own vectors on a very small corpus to understand vector similarity in a contextualized setting. We'll explore what vectors look like, how words are related, and the vocabulary of a model. \n",
    "* **Part 2**: GloVe word embeddings. GloVe is an unsupervised learning algorithm to calculate the vectors of the words in a corpus. We will use the pre-trained version that comes from Wikipedia articles. You can find [more about the GloVe vectors here](https://nlp.stanford.edu/projects/glove/). The goal of this section is to understand general word embeddings as trained on a very large corpus. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Notes\n",
    "You are not expected to understand the math or the code. We are using dimensionality reduction, which has a lot of uses including visualization and optimizing performance efficiency. For us, we are using dimensionality reduction for visualization. Word vectors create very large matrices. We can only visualize things on 2 or 3 dimension. So we reduce the dimensions. \n",
    "\n",
    "#### The book in the file\n",
    "[From wikipedia:](https://en.wikipedia.org/wiki/Catching_Fire) Catching Fire is a 2009 dystopian young adult fiction novel by the American novelist Suzanne Collins, the second book in The Hunger Games series. As the sequel to the 2008 bestseller The Hunger Games, it continues the story of Katniss Everdeen and the post-apocalyptic nation of Panem. Following the events of the previous novel, a rebellion against the oppressive Capitol has begun, and Katniss and fellow tribute Peeta Mellark are forced to return to the arena in a special edition of the Hunger Games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf1325",
   "metadata": {},
   "source": [
    "* `re` is the regular expression library\n",
    "* `gensim` is for representing documents as vectors. It's the library built for making word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb75a",
   "metadata": {},
   "source": [
    "## Part 1: Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA   \n",
    "from sklearn.manifold import TSNE   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd3f45",
   "metadata": {},
   "source": [
    "Load a file and replace the markup characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a file you have stored locally\n",
    "# I added the Hunger Games for simplicity\n",
    "file = open(\"hunger_games.txt\", 'r').read()\n",
    "\n",
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "    file = file.replace(char, \" \")\n",
    "\n",
    "# check\n",
    "print(file[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b44df1",
   "metadata": {},
   "source": [
    "Clean the text a little bit (tokenize, lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089918f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is simplified for demonstration\n",
    "def sample_clean_text(text: str):\n",
    "    # step 1: tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # step 2: tokenize each sentence into words\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # step 3: convert each word to lowercase\n",
    "    tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
    "    \n",
    "    # return your tokens\n",
    "    return tokenized_text\n",
    "\n",
    "# call the function\n",
    "tokens = sample_clean_text(text = file)\n",
    "\n",
    "# check\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4feaf1",
   "metadata": {},
   "source": [
    "Establish the [word2vec ](https://www.tensorflow.org/text/tutorials/word2vec#:~:text=word2vec%20is%20not%20a%20singular,downstream%20natural%20language%20processing%20tasks.)model. We've given it our list of tokens. We said that for each token, we want to define it in the context of 100 words. \n",
    "\n",
    "This means that each vector will have 100 numbers in it representing the relationship between this word and 100 others. \n",
    "\n",
    "The larger this number is, the better the model. The smaller the number, the worse the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d76ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens,vector_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d832a",
   "metadata": {},
   "source": [
    "Now explore the entire vocabulary (note that only 1000 are represented because of the interface -- there are far more words in the vocab). Use this for ideas in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55484ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833be5f",
   "metadata": {},
   "source": [
    "Now we pick a word from the corpus (remember this is lowercase) and see what the vector associated with it is. This is not human-readable, but it's this list (array) of numbers that helps us define the word in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22431ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.get_vector(\"capitol\", norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f78d5",
   "metadata": {},
   "source": [
    "Find all the words most similar to a word you've chosen. Notice that this similarity is not the same as the vector representation above. \n",
    "\n",
    "After you run this cell, make a note of the top 5 words. Go back and change the number of words in the vector and re-run the cells in between. Did it change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df90eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('capitol')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414634d",
   "metadata": {},
   "source": [
    "Now see how similar some words are. Change these words if you change the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aec383",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('katniss', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('peeta', 'home')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c25302",
   "metadata": {},
   "source": [
    "Finally visualize the result. There's a lot of code here, it's really just for visualization. The first part is the dimensionality reduction (using t-SNE). The second part puts it on a visualization. \n",
    "\n",
    "\n",
    "Spend more time exploring the visualization than breaking down the code. \n",
    "\n",
    "The visualization that results is very hard to see. If you are in a Jupyter Notebook, there are tools in the upper right corner of the cell that has the visualization in it (they appear on hover). You can zoom in on the visualization to see the words and the relationships between the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1823f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877d7ff",
   "metadata": {},
   "source": [
    "## Part 2 : GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f82ea",
   "metadata": {},
   "source": [
    "**BEFORE you move on, did you:**\n",
    "\n",
    "* DOWNLOAD the [Embeddings file for Part 2 here](https://drive.google.com/file/d/1NQ_7ERV3eiUnPCoIW7I8yi4TZMiu2cEQ/view?usp=sharing)\n",
    "* Unzip the file\n",
    "* Move it to the folder where you are writing your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866249f",
   "metadata": {},
   "source": [
    "There are a few files in the `glove/6B` folder. These are the embeddings. You can open them and see them in any text editor. 50d is the smallest one. Each word has 50 dimensions. This means it is defined in terms of 50 other words. \n",
    "\n",
    "This lab uses the 100d file. If it's running slower than you like, feel free to use 50d instead. If you want more accurate word relationships, use one of the larger files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28914b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish an empty dictionary\n",
    "embeddings_dict = {}\n",
    "\n",
    "#open the file and read it into the dictionary\n",
    "with open(\"glove.6B/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the Euclidean distance between the vectors for words and 1 or more other words.\n",
    "#sort the resulting word distances.\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: \n",
    "                  spatial.distance.euclidean(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fb5f6",
   "metadata": {},
   "source": [
    "Now try it out - pick some words to experiment with. This experimentation section is the most important part of Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"dog\"]\n",
    ")[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"cat\"]\n",
    ")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9643e",
   "metadata": {},
   "source": [
    "Now pick 2 words and see how it changes. Note that the `+` actually indicates that the vectors are being added. This may have slightly unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af30179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"dog\"] + embeddings_dict[\"cat\"]\n",
    ")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bb107",
   "metadata": {},
   "source": [
    "See what happens with a 3rd. These embeddings are basically being triangulated - and this list is what is nearest to the center of these 3 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_embeddings(\n",
    "    embeddings_dict[\"dog\"] + embeddings_dict[\"cat\"] + embeddings_dict[\"pet\"]\n",
    ")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbfb28",
   "metadata": {},
   "source": [
    "Now transform the dictionary into an array so that we can do dimensionality reduction so we can visualize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1465e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  list(embeddings_dict.keys())\n",
    "vectors = [embeddings_dict[word] for word in words]\n",
    "X = np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896083b",
   "metadata": {},
   "source": [
    "Now do the dimensionality reduction with t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa924c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "Y = tsne.fit_transform(X[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6ecbe",
   "metadata": {},
   "source": [
    "Plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65115b3",
   "metadata": {},
   "source": [
    "Annotate a few of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, x, y in zip(words, X[:, 0], Y[:, 1]):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75858550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
