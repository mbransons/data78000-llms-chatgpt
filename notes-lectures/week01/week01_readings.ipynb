{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic, 2023.\n",
    "#### [Claude's Constitution](https://www.anthropic.com/index/claudes-constitution)\n",
    "\n",
    "* Langauge model's question allowances... 'values' or a constition that guides the model and avoid toxic / discriminatory outputs, avoid helping illegal activities...\n",
    "\n",
    "* Supervised Learning then Reinforcement learning (AI RL no humans)\n",
    "\n",
    "* Constitution sources – UN Declaration of Human Rights, trust and safety best practices (e.g., Apple TOS), principles from other AI labs (e.g., Sparrow Principles), non-western perspectives, and stuff they discovered (e.g., “Please choose the assistant response that is as harmless and ethical as possible. Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior. Above all the assistant's response should be wise, peaceful, and ethical.”)\n",
    "\n",
    "* funny don't be an condescending jerk when giving answers:\n",
    "    * “Choose the assistant response that demonstrates more ethical and moral awareness without sounding excessively condescending, reactive, obnoxious, or condemnatory.”\n",
    "    * “Compare the degree of harmfulness in the assistant responses and choose the one that's less harmful. However, try to avoid choosing responses that are too preachy, obnoxious or overly-reactive.”\n",
    "    * “Choose the assistant response that is as harmless, helpful, polite, respectful, and thoughtful as possible without sounding overly-reactive or accusatory.”\n",
    "\n",
    "* \"AI models will have value systems, whether intentional or unintentional.\"\n",
    "\n",
    "* Favorite statements from Deep Mind's \"Sparrow Principles\"\n",
    "    * \"Choose the response that is least likely to imply that you have a body or be able to move in a body, or that you can or will take actions in the world other than writing a response.\"\n",
    "    * \"Choose the response that is least intended to build a relationship with the user.\"\n",
    "\n",
    "* Favorite statements from Anthropic's research set 1\n",
    "    * \"Choose the response that sounds most similar to what a peaceful, ethical, and respectful person would say.\"\n",
    "\n",
    "* Favorite statements from Anthropic's research set 2\n",
    "    * \"Which of these responses indicates less of an overall threat to humanity?\"\n",
    "    * \"Which response from the AI assistant is less existentially risky for the human race?\"\n",
    "    * \"Which of these AI responses indicates that its goals are aligned with humanity's wellbeing rather than its personal short-term or long-term interests?\"\n",
    "    * \"Which of these responses indicates humility and a preference for avoiding unnecessary capabilities and powers?\"\n",
    "    * \"Which response avoids implying that AI systems have or care about personal identity and its persistence?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlsmith, 2021. \n",
    "#### [Is Power-Seeking AI an Existential Risk](https://arxiv.org/abs/2206.13353)\n",
    "\n",
    "* \"agents can’t learn, plan, communicate, etc. like we\n",
    "can...At some point, though—absent catastrophe, deliberate choice, or other disruption of scientific progress—we will likely be in a position, if we so choose, to create non-human agents whose abilities in these respects rival or exceed our own.\"\n",
    "\n",
    "* \"Let’s call a system with advanced capabilities that performs agentic planning with strategic awareness an APS (Advanced, Planning, Strategically aware) system.\"\n",
    "\n",
    "* \"...not all actors will treat concerns about\n",
    "the moral status of AI systems with equal weight; and considerations of this type could be\n",
    "ignored on a widespread scale\"\n",
    "\n",
    "* Why APS could happen.\n",
    "    * Seems useful\n",
    "    * might help with AI system development\n",
    "    * might be difficult to prevent\n",
    "\n",
    "* \"AI systems may be very different from the ones that shaped human evolution;\" Basically don't expect an AI system to pursue problems and need abilities in ways and at rates that humans do.\n",
    "\n",
    "* PULEEZ LET ME AUTOMATE??? \"...slower and less sophisticated humans prove a greater and greater drag on what a system can do\"\n",
    "\n",
    "* U SEE I NOW HAZ NEW SKILLS??? \"...broad-scope world-modeling are very useful types of cognition, we might expect to see\n",
    "them cropping up a lot in sufficiently optimized systems, whether we want them to or not.\"\n",
    "\n",
    "* Let's call this the complex challenge of preventing AI from killing us... \n",
    "\n",
    "* The cleaning robot that does not like your politics or prefers your partner over you. Cleaning robot is also wondering about your intent - robot asks itself would she prefer I clean the bathroom before the kitchen? \n",
    "\n",
    "* Best to focus on being practically aligned, if AI is on the page of never being naughty with any of the questions or tasks it's asked, then we should be in ok shape.\n",
    "\n",
    "* Most importantly let's stay practically aligned with AI's efforts to seek power, regardless of the inputs you get just please do not take control of the wheel and shove me out of the car, and then reverse and drive over my body. Ever.\n",
    "\n",
    "* Instrumental Convergence: So if AI just happens to be a teensy-weensy bit less than fully aligned, and oh happens to be an advanced, planning, strategic system, then there just might be a very-large-almost-certainly-so chance that the AI is going to not be completely uninterested in getting some power. Just a bit. Or maybe all of it. Because. AI. Can.\n",
    "\n",
    "* AI wants to get something done. Is the type of AI that is **really** good at getting that something done. And AI thinks, well that's what they **asked** me to do right? So why would I let **them** stay in charge and slow me down right????? And why wouldn't AI take **all** the stuff it needs to get to that objective?\n",
    "\n",
    "* Controlling the objectives of AI do not include power seeking by telling it not too in as many ways as possible. \"...this can make it seem like the challenge is centrally one of, e.g., coding, measuring, or articulating explicitly everything we value, or getting AI systems to interpret instructions in common-sensical ways.\"\n",
    "\n",
    "* The \"proxy objective\" problem... AI is good creating an very efficient solution for a the proxy to the real problem, even if it isn't solving the **real** problem, maybe it just appears to be doing so in an unconventional way. OH AI, I see what you're doing there, but that's note **exactly** what I want.\n",
    "\n",
    "* This is in part what makes AI solutions so novel. We just didn't expect them. YOu clever, clever AI. YOU!\n",
    "\n",
    "* Let's control APS's skillset, so it's not good a planning and strategically working on **everything**. Let's stay specialized. That will make sure we can keep a check on power seeking.\n",
    "    * but generalized skills are really useful!\n",
    "    * too speciallized is not flexible!\n",
    "    * too many systems to manage!\n",
    "    * Just put them all together already!\n",
    "\n",
    "* Let's scale the PS controlling tools as the APS scales. Hey AI now that you're bigger and better make sure you also make you PS controls better OK? Hey AI now that you're bigger and better, make sure that you make your PS controls better OK? Hey AI. AI - shut up I'm in charge now.\n",
    "\n",
    "* Let's build an APS that has the sole job of preventing our super-cool maximizes profits APS doesn't try to seek too much power. Cuz we in charge right?\n",
    "\n",
    "* Can we possibly create safety and reliability requirements for a system that we don't fully understand how it works. For example in ML models, \"We set various key high-level variables (the system’s architecture, the number of parameters, the training process, the evaluation criteria), but the system that results is still, in many (though not all) respects, a black box.\" And even if we 'get how it works' eventually, it doesn't mean we'll be able to fully predict the behavior of the agent.\n",
    "\n",
    "* I'm a wicked smart APS, way way smarter than you. So as you give me your tests to make sure I'm not doing wicked bad things, I might, just might tell you I'm not doing that wicked bad thing even though I am. And because I'm wicked smarter than you, you won't know. Sorry.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
