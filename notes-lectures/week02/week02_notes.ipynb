{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The foundations: Ngrams, Semantic Embeddings, Tokenization\n",
    "\n",
    "Ngrams are the beginning, a probability model, word counts\n",
    "Semantic Embeddings also known as word vectors\n",
    "\n",
    "for fun \"Today is the first day of ...\"\n",
    "fill in the blank\n",
    "\n",
    "LM is simply words and probabilities...\n",
    "\n",
    "N-gram language model has the goal of predict the probability of a sentence\n",
    "\n",
    "Determiners - \"the\" and \"my\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\t\t\t\t\t\tRewrites as\n",
    "\n",
    "The probability of A and B (intersection) is the probability of A times the probability of B given A\n",
    "\n",
    "P(“it was the best of times”) =\n",
    "\tP(it) × P(was|it) ×  P(the|it was) ×  P(best|it was the) \n",
    "×  P(of|it was the best) ×  P(times|it was the best of)\n",
    "\n",
    "The probability of B given A \n",
    "\n",
    "P(B|A)\n",
    "\n",
    "The probability of the next word based on the previous fragment.\n",
    "\n",
    "- The chain rule (basically multipy each thing you know)\n",
    "\n",
    "\"Today is the first day of ...\"\n",
    "\n",
    "\"...the rest of your life\"\n",
    "\n",
    "four examples out of eight had \"the\" as the next word (50% probability)\n",
    "\n",
    "of those for examples each had a different next word (25%)\n",
    "\n",
    "the probability of any of those next two words is (12.5%)\n",
    "\n",
    "- Markov Assumption – the future depends only the the present. This is built off of a corpus for comparison. Probability of a word after that word in a specific corpus...\n",
    "\n",
    "\"Today is the first day of ...\" \n",
    "\n",
    "Stop words suck, so we deal with them\n",
    "\n",
    "n-grams are bigram one context word and one predicted word\n",
    "trigram two context words and one predicted word\n",
    "\n",
    "- Maximum Likelihood Estimation (MLE) very good at doing work in a limited corpus and making probabilities that are similar \n",
    "\tGoogle scanning all books and their n-grams project which is a massive corpus\n",
    "\tcontext is the individual word and all multiplied instances of all the words that preceded it.\n",
    "\n",
    "- probability notation\n",
    "\tP(y|x) = the probability of y given x.\n",
    "\tIf we know x, then what is the probaility of y happening?\n",
    "\tP(y|x) will be written P(x -> y) - we know x, what's y going to be.\n",
    "\n",
    "- Out of Vocabulary (OOV)\n",
    "\t0 probability to all ngrams it hasn't seen\n",
    "\tsolution is to mess with the probabilities (add 1 smoothing)\n",
    "\textremely low probability bigrams become like the ```<UNK>``` probability\n",
    "- The way these Ngram models are presented as deterministic (absolute), but randomness is intentionally introduced to allow variation\n",
    "\n",
    "- Tokenization must match the model's use of tokens\n",
    "\twhite space sucks because sees \"model\" and \"models\" as different tokens\n",
    "\tlinguistically aware (stemming and lemmatizing) - lemmas are the root words\n",
    "\tbyte pair encoding is the modern way... starts with letters and builds them up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
