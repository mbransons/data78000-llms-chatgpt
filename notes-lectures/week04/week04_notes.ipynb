{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "[Slides](https://docs.google.com/presentation/d/15rN-22sNiforAmAcdr8KbAEtevV-GMianXs1ykgHA_I/edit#slide=id.g27f9598b9d2_0_414)\n",
    "\n",
    "- define a word by the company it keeps\n",
    "\n",
    "- words in our corpus define a multi-dimensional space, each word can be defined based on it's proximity to every other word in the corpus\n",
    "    - so a vector is an encoding co-occurence with other words for \"all\" words in the corpus\n",
    "- duck and pond share a ton of numbers, but we're not saying they can be substituted for one another\n",
    "\n",
    "#### Word2vec\n",
    "\n",
    "- for each word we care about 50 words around it, or 100 words around it, to be determined\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "[Neural Network Slides](https://docs.google.com/presentation/d/1UK78GyfIbuyjkBshmboga43tjfKOP_jWsZJW3O3m5H8/edit#slide=id.p)\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "- vs. linear regression which is a prediction algorithm.\n",
    "- it's a classifying algorithm\n",
    "- supervised vs. unsupervised vs. reinforcement (hybrid)\n",
    "- supervised has a classification label built into the data\n",
    "- can only return a positive vs. a negative (spam or ham)\n",
    "- sigmoid function to squish all the values between 0 and 1. It's logrithmic so outliers are flattened and are close to 1 or 0.\n",
    "- probability here is not as we would normally think of proababilities. The number is simply to define the content of a threshold.\n",
    "- activation function is the point at which the threshold is passed.\n",
    "- upsupervised does not have predefined labels (k means clustering)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
