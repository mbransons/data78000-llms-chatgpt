{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of LLMs\n",
    "* Phatic communion is a description of language that is used to hold together social contexts and communities. The how you say things not what you say... \n",
    "* This is something that ChatGPT does as a chatbot but most LLM tools do not.\n",
    "\n",
    "#### GLUE based leaderboard is likely the real start of the LLM \"arms-race\"\n",
    "\n",
    "* Accuracy - ability to get things that are true as true and getting things that are false, false.\n",
    "The accuracy metric in the context of GLUE evaluates the model's ability to correctly predict the labels or outcomes associated with the given tasks. The specific calculation of accuracy depends on the nature of the individual task within the GLUE benchmark.\n",
    "\n",
    "For example, in a classification task, accuracy is often computed as the ratio of correctly predicted instances to the total number of instances. It can be expressed as:\n",
    "\n",
    "Accuracy = Number of Correct Predictions/Total Number of Instances\n",
    "\n",
    "This formula holds for tasks where the goal is to classify input into two or more categories. However, some tasks in GLUE may have different evaluation metrics tailored to their specific requirements. For instance, tasks such as the Stanford Question Answering Dataset (SQuAD) may use metrics like F1 score and Exact Match to evaluate the model's performance in answering questions.\n",
    "\n",
    "* Test types:\n",
    "    * Single Sentence Acceptability\n",
    "    * Similarity & Paraphrase - Binary (T/F)\n",
    "    * Inference Tasks - Entailment, Contradiction, Neutral\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
